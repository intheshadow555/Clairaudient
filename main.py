import requests
import time
import datetime
import random
import os
from dotenv import load_dotenv
load_dotenv()

class HakoneEkidenMonitor:
    def __init__(self):
        self.keyword = "ç®±æ ¹é©¿ä¼ "
        self.api_url = "https://api.bilibili.com/x/web-interface/search/type"

        # ã€é‡è¦ã€‘å¡«å…¥ Cookie
        self.cookie = os.environ.get("BILIBILI_COOKIE") 
        
        if not self.cookie:
            print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Cookieï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶æˆ– GitHub Secrets è®¾ç½®ã€‚")

        # æ¨¡æ‹Ÿæµè§ˆå™¨å¤´éƒ¨ï¼Œé˜²æ­¢è¢«Bç«™æ‹¦æˆª
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://search.bilibili.com/",
            "Cookie": self.cookie  # å¸¦ä¸Šèº«ä»½è¯
        }

    def fetch_videos(self, order_type, limit):
        """
        è·å–è§†é¢‘åˆ—è¡¨
        :param order_type: æ’åºæ–¹å¼ ('pubdate': æœ€æ–°å‘å¸ƒ, 'click': æœ€å¤šæ’­æ”¾, 'scores': æœ€å¤šè¯„è®º, 'stow': æœ€å¤šæ”¶è—, 'damku': æœ€å¤šå¼¹å¹•, 'default': ç»¼åˆæ’åº)
        :param limit: è·å–æ•°é‡
        """
        params = {
            "search_type": "video",
            "keyword": self.keyword,
            "order": order_type,
            "page": 1,
            "page_size": 20
        }

        try:
            # æ‰“å°æ­£åœ¨è¯·æ±‚çš„ URLï¼Œæ–¹ä¾¿è°ƒè¯•
            print(f"ğŸ“¡ è¯·æ±‚ Bç«™æ¥å£ (Order: {order_type})...")

            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)

            # æ£€æŸ¥ HTTP çŠ¶æ€ç 
            if response.status_code != 200:
                print(f"âŒ HTTP è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []

            # å°è¯•è§£æ JSON
            try:
                data = response.json()
            except ValueError:
                print("âŒ è§£æå¤±è´¥ï¼šBç«™è¿”å›çš„ä¸æ˜¯ JSON æ•°æ®ã€‚å¯èƒ½æ˜¯è¢«æ‹¦æˆªäº†ï¼Œè¯·æ£€æŸ¥ Cookie æ˜¯å¦è¿‡æœŸã€‚")
                print(f"è¿”å›å†…å®¹å‰100å­—: {response.text[:100]}")  # æ‰“å°å‡ºæ¥çœ‹çœ‹æ˜¯å•¥
                return []

            # æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç 
            if data['code'] != 0:
                print(f"âŒ API ä¸šåŠ¡æŠ¥é”™: {data['message']}")
                return []

            video_list = []
            if 'data' in data and 'result' in data['data']:
                items = data['data']['result']
                for item in items:
                    # åªæœ‰å½“ type ä¸º video æ—¶æ‰æå–ï¼ˆæœ‰æ—¶å€™ä¼šæ··å…¥å…¶ä»–å†…å®¹ï¼‰
                    if item.get('type') == 'video':
                        video_info = {
                            "title": item['title'].replace('<em class="keyword">', '').replace('</em>', ''),
                            "author": item['author'],
                            "play": item['play'],
                            # å¤„ç†æ—¶é—´æˆ³ï¼Œé˜²æ­¢æŠ¥é”™
                            "date": datetime.datetime.fromtimestamp(item['pubdate']).strftime('%Y-%m-%d'),
                            "url": f"https://www.bilibili.com/video/{item['bvid']}",
                            "bvid": item['bvid']
                        }
                        video_list.append(video_info)

            print(f"âœ… æˆåŠŸè·å– {len(video_list)} æ¡æ•°æ®")
            return video_list[:limit]

        except Exception as e:
            print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _parse_count(self, val):
        """æŠŠ API è¿”å›çš„æ’­æ”¾/ç‚¹å‡»/æ”¶è—ç­‰æ–‡æœ¬æ•°å€¼å½’ä¸€åŒ–ä¸ºæ•´æ•°ã€‚"""
        if val is None:
            return 0
        try:
            if isinstance(val, (int, float)):
                return int(val)
            s = str(val).strip()
            # å¤„ç†å¸¦å•ä½çš„ä¸­æ–‡æ•°å­—ï¼ˆå¦‚ 1.2ä¸‡ï¼‰
            if 'äº¿' in s:
                return int(float(s.replace('äº¿', '')) * 100000000)
            if 'ä¸‡' in s:
                return int(float(s.replace('ä¸‡', '')) * 10000)
            # ç§»é™¤é€—å·å’Œéæ•°å­—å­—ç¬¦
            s2 = ''.join(ch for ch in s if (ch.isdigit() or ch == '.'))
            if s2 == '':
                return 0
            return int(float(s2))
        except Exception:
            return 0

    def fetch_week_videos_top_by_click(self, limit=10, max_pages=10, page_size=50):
        """
        æŠ“å–æœ€è¿‘ä¸€å‘¨ï¼ˆ7å¤©ï¼‰å†…æ‰€æœ‰ä¸å…³é”®è¯åŒ¹é…çš„è§†é¢‘ï¼ŒæŒ‰æ’­æ”¾é‡ï¼ˆclick/playï¼‰æ’åºå¹¶è¿”å›å‰ limit ä¸ªã€‚
        """
        end_time = int(time.time())
        start_time = end_time - 7 * 24 * 60 * 60

        collected = []
        seen = set()

        for page in range(1, max_pages + 1):
            params = {
                "search_type": "video",
                "keyword": self.keyword,
                "order": 'pubdate',
                "page": page,
                "page_size": page_size
            }

            try:
                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)
                if response.status_code != 200:
                    break
                data = response.json()
                if data.get('code') != 0:
                    break
                items = data.get('data', {}).get('result', [])
                if not items:
                    break

                # åˆ¤æ–­æœ¬é¡µæ˜¯å¦å…¨éƒ¨æ—©äº start_timeï¼Œç”¨äºæå‰åœæ­¢
                all_older = True
                for item in items:
                    if item.get('type') != 'video':
                        continue
                    pub = item.get('pubdate', 0)
                    if pub >= start_time:
                        all_older = False
                        bvid = item.get('bvid')
                        if not bvid or bvid in seen:
                            continue
                        seen.add(bvid)
                        play = self._parse_count(item.get('play') or item.get('click'))
                        video_info = {
                            'title': item.get('title', '').replace('<em class="keyword">', '').replace('</em>', ''),
                            'author': item.get('author', ''),
                            'play': play,
                            'date': datetime.datetime.fromtimestamp(pub).strftime('%Y-%m-%d'),
                            'url': f"https://www.bilibili.com/video/{bvid}",
                            'bvid': bvid,
                            # å¤‡ç”¨å­—æ®µï¼Œåç»­è®¡ç®—å¯ç”¨
                            'click': self._parse_count(item.get('click') or item.get('play')),
                            'scores': self._parse_count(item.get('reviews')),
                            'stow': self._parse_count(item.get('favorites'))
                        }
                        collected.append(video_info)

                if all_older:
                    break

            except Exception:
                break

        # æŒ‰æ’­æ”¾é‡æ’åºå¹¶è¿”å›å‰ limit
        collected.sort(key=lambda x: x.get('play', 0), reverse=True)
        return collected[:limit]

    def fetch_top_click_candidates(self, candidate_count=20, max_pages=5, page_size=50):
        """
        æŒ‰ click æ’åºæŠ“å–ï¼ˆAPI order=clickï¼‰ï¼Œè¿”å›å»é‡åçš„å‰ candidate_count æ¡å€™é€‰ï¼Œç”¨äºåç»­è¯„åˆ†ã€‚
        """
        collected = []
        seen = set()
        page = 1
        while len(collected) < candidate_count and page <= max_pages:
            params = {
                "search_type": "video",
                "keyword": self.keyword,
                "order": 'click',
                "page": page,
                "page_size": page_size
            }
            try:
                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)
                if response.status_code != 200:
                    break
                data = response.json()
                if data.get('code') != 0:
                    break
                items = data.get('data', {}).get('result', [])
                if not items:
                    break
                for item in items:
                    if item.get('type') != 'video':
                        continue
                    bvid = item.get('bvid')
                    if not bvid or bvid in seen:
                        continue
                    seen.add(bvid)
                    play = self._parse_count(item.get('play') or item.get('click'))
                    info = {
                        'title': item.get('title', '').replace('<em class="keyword">', '').replace('</em>', ''),
                        'author': item.get('author', ''),
                        'play': play,
                        'date': datetime.datetime.fromtimestamp(item.get('pubdate', 0)).strftime('%Y-%m-%d'),
                        'url': f"https://www.bilibili.com/video/{bvid}",
                        'bvid': bvid,
                        'click': self._parse_count(item.get('click') or item.get('play')),
                        'scores': self._parse_count(item.get('scores')),
                        'stow': self._parse_count(item.get('stow'))
                    }
                    collected.append(info)
                    if len(collected) >= candidate_count:
                        break
                page += 1
            except Exception:
                break

        # å¦‚æœ API è¿”å›çš„å·²ç»æ˜¯æŒ‰ click æ’åºï¼Œè¿™é‡Œä»æŒ‰ click æ’åºç¡®ä¿ä¸€è‡´æ€§
        collected.sort(key=lambda x: x.get('click', 0), reverse=True)
        return collected[:candidate_count]

    def compute_weighted_hot(self, candidates, weights=(0.2, 0.5, 0.3), top_n=5):
        """
        å¯¹å€™é€‰é›†ä¾æ® click, scores, stow ä¸‰ä¸ªæŒ‡æ ‡æŒ‰ weights åŠ æƒï¼Œå½’ä¸€åŒ–åè®¡ç®—æ€»åˆ†å¹¶è¿”å› top_nã€‚
        weights: (w_click, w_scores, w_stow)
        """
        if not candidates:
            return []

        clicks = [c.get('click', 0) for c in candidates]
        scores = [c.get('scores', 0) for c in candidates]
        stows = [c.get('stow', 0) for c in candidates]

        def normalize(arr):
            mn = min(arr)
            mx = max(arr)
            if mx == mn:
                return [1.0 for _ in arr]
            return [(v - mn) / (mx - mn) for v in arr]

        n_click = normalize(clicks)
        n_scores = normalize(scores)
        n_stow = normalize(stows)

        w_click, w_scores, w_stow = weights

        for i, c in enumerate(candidates):
            score = n_click[i] * w_click + n_scores[i] * w_scores + n_stow[i] * w_stow
            c['_weighted_score'] = score

        candidates.sort(key=lambda x: x.get('_weighted_score', 0), reverse=True)
        return candidates[:top_n]

    def run(self):
        print(f"ğŸƒ å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šæœç´¢å…³é”®è¯ [{self.keyword}]...")

        if "ä½ çš„Bç«™Cookie" in self.cookie:
            print("âš ï¸ è­¦å‘Šï¼šä½ è¿˜æ²¡æœ‰å¡«å†™ Cookieï¼Œè·å–çƒ­é—¨è§†é¢‘æå¤§æ¦‚ç‡ä¼šå¤±è´¥ï¼")

        # 1. è·å– 10 ä¸ªæœ¬å‘¨å‘å¸ƒå¹¶æŒ‰æ’­æ”¾é‡æ’åºçš„æœ€æ–°è§†é¢‘
        print("\n--- æ­£åœ¨è·å–æœ¬å‘¨æ‰€æœ‰ç›¸å…³è§†é¢‘å¹¶æŒ‰æ’­æ”¾é‡æ’åºï¼Œå–å‰10 ---")
        new_videos = self.fetch_week_videos_top_by_click(limit=10)

        # ã€å…³é”®ä¿®æ”¹ã€‘å¢åŠ å»¶æ—¶ï¼Œé˜²æ­¢è¯·æ±‚å¤ªå¿«
        sleep_time = random.uniform(5, 8)
        print(f"\nğŸ’¤ ä¼‘æ¯ {sleep_time:.1f} ç§’ï¼Œé˜²æ­¢è¢« Bç«™å°é”...")
        time.sleep(sleep_time)

        # 2. è·å– 20 ä¸ªæŒ‰ click æ’åºçš„å€™é€‰è§†é¢‘ï¼ŒæŒ‰æŒ‡æ ‡å½’ä¸€åŒ–åŠ æƒåå–å‰5
        print("\n--- æ­£åœ¨è·å– click æ’åºå€™é€‰å¹¶è®¡ç®—åŠ æƒå¾—åˆ†ï¼Œå– top5 ---")
        candidates = self.fetch_top_click_candidates(candidate_count=20)
        hot_candidates = self.compute_weighted_hot(candidates, weights=(0.2, 0.5, 0.3), top_n=10)

        # å»æ‰ä¸æœ¬å‘¨æœ€æ–°è§†é¢‘é‡å¤çš„é¡¹ï¼Œæœ€ç»ˆå–å‰5
        new_ids = {v['bvid'] for v in new_videos}
        old_videos = [v for v in hot_candidates if v['bvid'] not in new_ids]
        old_videos = old_videos[:5]

        # 4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_report(new_videos, old_videos)

    def generate_report(self, new_list, old_list):
        report_lines = []
        report_lines.append(f"# ğŸ½ ç®±æ ¹é©¿ä¼ å‘¨æŠ¥ ({datetime.date.today()})")
        report_lines.append(f"æœ¬å‘¨ä¸ºæ‚¨æ±‡æ€»äº† **{len(new_list)}** ä¸ªæ–°è§†é¢‘å’Œ **{len(old_list)}** ä¸ªç»å…¸å›é¡¾ã€‚\n")

        report_lines.append("## ğŸ†• æœ¬å‘¨çƒ­é—¨å‘å¸ƒ (Top 10 by æ’­æ”¾é‡)")
        report_lines.append("| å‘å¸ƒæ—¥æœŸ | æ ‡é¢˜ | UPä¸» | æ’­æ”¾é‡ |")
        report_lines.append("|---|---|---|---|")
        for v in new_list:
            title_safe = v.get('title', '')
            title_safe = title_safe.replace('|', '&#124;').replace('\n', ' ').replace('\r', ' ')
            report_lines.append(f"| {v['date']} | [{title_safe}]({v['url']}) | {v['author']} | {v['play']} |")

        report_lines.append("\n## ğŸ”¥ ç»å…¸/çƒ­é—¨ (Hot 5, åŸºäº click/scores/stow åŠ æƒæ’åº)")
        report_lines.append("| å‘å¸ƒæ—¥æœŸ | æ ‡é¢˜ | UPä¸» | æ’­æ”¾é‡ | å¾—åˆ† |")
        report_lines.append("|---|---|---|---|---|")
        if not old_list:
            report_lines.append("| - | è·å–å¤±è´¥æˆ–æ— æ•°æ® | - | - | - |")
        for v in old_list:
            title_safe = v.get('title', '')
            title_safe = title_safe.replace('|', '&#124;').replace('\n', ' ').replace('\r', ' ')
            score = v.get('_weighted_score')
            score_str = f"{score:.3f}" if score is not None else "-"
            report_lines.append(f"| {v['date']} | [{title_safe}]({v['url']}) | {v['author']} | {v.get('play', 0)} | {score_str} |")

        content = "\n".join(report_lines)

        filename = "ç®±æ ¹é©¿ä¼ _report.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)

        print("-" * 30)
        print(f"âœ… æ±‡æ€»å®Œæˆï¼æ–‡ä»¶å·²ç”Ÿæˆ: {filename}")
        print("-" * 30)


if __name__ == "__main__":
    bot = HakoneEkidenMonitor()
    bot.run()